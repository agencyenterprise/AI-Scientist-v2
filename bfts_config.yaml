# path to the task data directory
data_dir: "data"
preprocess_data: False

goal: null
eval: null

log_dir: logs
workspace_dir: workspaces

# whether to copy the data to the workspace directory (otherwise it will be symlinked)
# copying is recommended to prevent the agent from accidentally modifying the original data
copy_data: True

exp_name: run # a random experiment name will be generated if not provided

# settings for code execution
exec:
  timeout: 3600
  agent_file_name: runfile.py
  format_tb_ipython: False

generate_report: True
# LLM settings for final report from journal
report:
  model: gpt-5-mini
  temp: 1.0

# LLM settings for paper writeup
writeup:
  big_model: gpt-5
  small_model: gpt-5-mini
  plot_model: gpt-5-mini

experiment:
  num_syn_datasets: 1

# Hardware/Compute Resources Available
compute:
  gpu:
    type: "RTX 4090"
    count: 2
    vram_gb: 24
  ram_gb: 124
  vcpu: 32
  notes: "Each experiment runs on ONE RTX 4090 GPU (24GB VRAM each). The pod has 2 GPUs total, 124GB system RAM, and 32 vCPUs, enabling parallel execution of 2 experiments/seeds simultaneously. Each GPU can handle moderate-sized models (~3B params for training, ~7B for inference), batch sizes of 16-64, and extensive training. With 124GB RAM, you can load large datasets directly into memory without streaming in most cases. Fine-tuning is fully supported - the agent can load pre-trained models from HuggingFace and fine-tune them using standard methods (full fine-tuning, LoRA, etc). DATASETS: Use dataset sizes appropriate to the experiment - with 124GB RAM, you can handle very large datasets. Only use streaming=True for massive datasets (>50GB) that exceed RAM capacity."

debug:
  stage4: False

# agent hyperparams
agent:
  type: parallel
  num_workers: 10
  stages:
    stage1_max_iters: 16
    stage2_max_iters: 10
    stage3_max_iters: 8
    stage4_max_iters: 14
  # how many improvement iterations to run
  steps: 5 # if stage-specific max_iters are not provided, the agent will use this value for all stages
  # whether to instruct the agent to use CV (set to 1 to disable)
  k_fold_validation: 1
  multi_seed_eval:
    num_seeds: 2 # should match GPU count for optimal parallel execution. With 2 GPUs, use 2 seeds to run them in parallel.
  # whether to instruct the agent to generate a prediction function
  expose_prediction: False
  # whether to provide the agent with a preview of the data
  data_preview: False

  # LLM settings for coding
  code:
    model: gpt-5
    temp: 1.0
    max_tokens: 12000

  # LLM settings for evaluating program output / tracebacks
  feedback:
    model: gpt-5
    # gpt-4o
    temp: 0.5
    max_tokens: 8192

  vlm_feedback:
    model: gpt-5
    temp: 0.5
    max_tokens: null

  search:
    max_debug_depth: 5  # increased from 3 - try harder to debug
    debug_prob: 0.7     # increased from 0.5 - debug more often
    num_drafts: 3
