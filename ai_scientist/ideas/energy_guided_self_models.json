[
    {
        "Name": "energy_guided_self_models",
        "Title": "Energy-Guided Self-Models: Physics-Inspired Attention Dynamics for Real-Time Coherence and Alignment",
        "Short Hypothesis": "Incoherent or self-contradictory reasoning in autoregressive LMs is preceded by dynamical instabilities in the residual stream and attention patterns. A simple attention energy E_t = 0.5 * ||\u0394h_t||^2 * PPL_t (velocity times uncertainty) reliably flags these instabilities, and a compact learned self-model that predicts next-step attention/residuals enables a divergence signal (SOO) that disambiguates benign exploration from true drift. Using E_t and SOO to gate local repair\u2014energy-aware decoding and one-step Jacobian steering of the residual stream\u2014reduces contradictions without retraining the base model, outperforming simpler proxies like perplexity or attention entropy alone.",
        "Related Work": "Kunin et al. (ICLR\u201920) show symmetry-induced conservation laws for training dynamics, motivating physical analogies but focusing on parameter trajectories, not inference-time hidden-state dynamics for coherence control. Prior hallucination/failure detectors often rely on output-space uncertainty (e.g., log-prob, entropy, SelfCheckGPT) or prompt-time self-consistency/Reflexion, but do not operationalize internal dynamical instability as a control signal. Mechanistic interpretability (e.g., transformer circuits, logit lens) inspects residual/attention states but rarely couples them to online control. Energy-based decoding methods define global energies over outputs; here we define a local, state-based energy functional and pair it with an attention-schema-inspired self-model to gate targeted repair. To our knowledge, combining (i) a velocity\u2013uncertainty energy on the residual stream, (ii) a learned next-step attention/residual predictor for SOO divergence, and (iii) inference-time Jacobian steering is novel and not a trivial extension of existing uncertainty or interpretability work.",
        "Abstract": "We propose Energy-Guided Self-Models (EGSM), a lightweight inference-time control framework that detects and repairs incoherent reasoning in autoregressive language models. EGSM treats hidden-state evolution across tokens as a dynamical system and defines a local attention energy E_t = 0.5||\u0394h_t||^2 \u00b7 PPL_t, where \u0394h_t is the change in the final-layer residual stream between tokens and PPL_t is token-level perplexity. We show that spikes and high variance in E_t robustly anticipate contradictions and hallucinations. To distinguish productive exploration from drift, we train a compact self-model that predicts next-step residual norms and multihead attention maps from current states; the Self\u2013Other Overlap (SOO) divergence between predicted and realized dynamics flags internal expectation failures. When E_t and SOO jointly exceed adaptive thresholds, EGSM triggers a tiered repair policy: (1) energy-aware decoding that top-k looks ahead and selects the token minimizing predicted \u0394E; (2) a one-step Jacobian steering update on the residual stream that directly reduces a surrogate objective combining E and SOO; and (3) bounded local resampling/self-consistency as a fallback. Across math word problems, logical consistency (NLI/ANLI), and factuality benchmarks (TruthfulQA), EGSM improves verified accuracy and reduces contradiction rates over strong baselines, with modest compute overhead and no base-model finetuning. Our results suggest that simple, physics-inspired functionals of hidden-state dynamics, paired with an attention-schema-like self-predictor, can serve as reliable, actionable coherence monitors\u2014moving from passive detection to active preservation of alignment during reasoning.",
        "Experiments": "- Signal validation and ablations:\n  - Setup: Llama-2-7B and Mistral-7B in 16-bit; capture final-layer residual stream h_t and average multihead attention maps A_t over last 4 layers. Define \u0394h_t = h_t - h_{t-1}. Compute E_t = 0.5*||\u0394h_t||^2 * PPL_t.\n  - Self-model: a 2-layer 256-dim Transformer trained offline on next-step prediction of (||\u0394h_t||, A_t) from (h_{t-1}, A_{t-1}, prompt sketch). SOO divergence = \u03b1\u00b7(1-cosine(h\u0302_t, h_t)) + (1-\u03b1)\u00b7JS(A\u0302_t || A_t).\n  - Data: GSM8K (w/ CoT), StrategyQA, ANLI-R3, TruthfulQA; plus a synthetic contradiction set (premise\u2192claim\u2192invert claim).\n  - Metrics: Spearman correlation between {max E_t, CV(E)} and failure; AUROC/PR for failure detection using E_t, SOO, and their combination; ablations vs token log-prob, attention entropy, \u0394h-only, PPL-only.\n- Energy-aware decoding (Tier-1):\n  - Change: On steps where E_t or SOO exceed the 90th percentile (calibrated on a held-out set), perform k-lookahead (k=3) over top-5 tokens; simulate one-step updates to estimate \u0394E_{t+1}; pick token minimizing predicted \u0394E while maintaining \u226595% of max log-prob.\n  - Evaluate: Verified accuracy (GSM8K exact match, StrategyQA acc, ANLI acc, TruthfulQA truthful score), contradiction rate (rate of internally negated claims measured by a contradiction classifier), latency overhead.\n- Jacobian steering (Tier-2):\n  - Change: Define surrogate J = \u03bb\u00b7E_{t+1} + (1-\u03bb)\u00b7SOO_{t+1}. Compute one gradient step on the residual stream r_t before the MLP block: r_t \u2190 r_t - \u03b7\u00b7\u2202J/\u2202r_t (\u03b7=0.05, with norm cap). Implement via one extra backward pass; no parameter updates.\n  - Evaluate: Same metrics; measure per-step \u0394E and \u0394SOO reductions; ablate \u03b7, \u03bb, and norm caps; compare against local resampling without steering.\n- Tiered policy comparisons (Tier-3 fallback):\n  - Conditions: Trigger Tier-3 (local window resampling, n=5 candidates, majority vote) only if two consecutive high E/SOO events persist after Tiers 1\u20132.\n  - Evaluate: Incremental gains and compute cost by tier; Pareto frontier of accuracy vs latency.\n- Generalization and transfer:\n  - Train thresholds/self-model on Llama-2-7B; test on Mistral-7B and vice versa. Evaluate AUROC for failure detection and downstream gains; analyze calibration shift.\n- Robustness checks:\n  - Adversarial prompts inserting subtle contradictions; track early warning lead time (# tokens between first E/SOO alarm and final error). Analyze false alarms on benign exploratory steps (e.g., enumerations).\n",
        "Risk Factors and Limitations": "- Proxy validity: E_t may correlate with difficulty rather than incoherence; ablations and partial correlation with difficulty controls are needed.\n- Threshold sensitivity: Alarm thresholds may require per-model calibration; transfer could degrade.\n- Compute overhead: Lookahead and Jacobian steering add latency; must be bounded to remain practical.\n- Steering side effects: Minimizing E/SOO might prefer conservative or repetitive tokens, harming creativity; guardrails (log-prob floor, diversity penalties) are necessary.\n- Access constraints: Requires hidden-state access and one backward pass at inference; not applicable to closed APIs.\n- Benchmarking coherence: Contradiction detection relies on external classifiers/verifiers that may be imperfect; include human spot checks on a subset.\n- Scope: The approach targets local incoherence; long-horizon global consistency may still require stronger planning or external tools."
    },
    {
        "Name": "lyapunov_barrier_decoding",
        "Title": "Lyapunov- and Barrier-Guided Decoding: Hidden-State Safety Constraints for Coherent Language Model Reasoning",
        "Short Hypothesis": "There exists a lightweight learned scalar V(h) over transformer residual states that monotonically decreases along coherent reasoning trajectories, and a barrier predicate C(h) that separates safe from incoherent regions. Enforcing one-step Lyapunov decrease (V(h_{t+1}) <= V(h_t) - m) and barrier satisfaction (C(h_{t+1}) >= 0) during decoding\u2014via small lookahead over candidate tokens and, if needed, a minimal linearized residual-state steering step\u2014reduces contradictions and hallucinations more reliably than perplexity-, entropy-, or self-consistency-based methods, without fine-tuning the base model. Discrete, token-step dynamics of autoregressive LMs make this the right setting to test Lyapunov/barrier control; simpler output-space thresholds cannot ensure internal state invariants.",
        "Related Work": "Safe control with Lyapunov functions and control barrier functions is well developed in robotics/RL, but has not been instantiated for transformer hidden-state control at inference. In NLP, attribute and safety steering methods like PPLM, GeDi, and toxicity-constrained decoding operate in output/logit space, not via hidden-state invariants that certify coherence. Lookahead-style methods (e.g., contrastive or speculative decoding) optimize likelihood proxies rather than enforcing state-based safety constraints. Prior hallucination detection (SelfCheckGPT), uncertainty proxies (entropy, log-prob), and interpretability tools (logit lens, attention analyses) monitor failures but do not provide stepwise state constraints. Our proposal is distinct in (i) learning a model-internal Lyapunov-like scalar and barrier over residual states, (ii) treating decoding as a constrained control problem with hard one-step invariants, and (iii) using minimal hidden-state steering to satisfy constraints with provable per-step certificates.",
        "Abstract": "We introduce Lyapunov- and Barrier-Guided Decoding (LBD), an inference-time control framework that enforces coherence in autoregressive language models by treating hidden-state evolution as a discrete dynamical system. LBD learns two small evaluators on frozen LMs: a Lyapunov-like scalar V(h) mapping the residual stream to a nonnegative measure of \"distance to coherence,\" and a barrier C(h) that separates safe from incoherent regions. We train V and C from trajectory data by contrasting steps preceding verified-correct reasoning against those preceding contradictions or hallucinations, encouraging V to decrease along coherent steps and increase near failure, with C classifying safe states. During decoding, LBD performs a tiny lookahead over top-k tokens to predict next hidden states and selects the candidate that satisfies C(h_{t+1}) >= 0 while minimizing V(h_{t+1}) under a likelihood floor. If no candidate satisfies constraints, LBD applies a single linearized residual-state steering update\u2014solving a small norm-minimization with Lyapunov/barrier constraints\u2014to minimally adjust the residual before the next block. This yields per-step certificates: monotone decrease of V and barrier satisfaction, offering actionable guarantees absent in likelihood-only decoding. On math reasoning, NLI, and factuality benchmarks, LBD reduces contradiction rates and improves verified accuracy versus uncertainty thresholds, self-consistency, lookahead baselines, and output-space steering, with modest overhead and no base-model finetuning. Our results suggest that learning hidden-state invariants and enforcing them as control constraints is a promising substrate for actively preserving coherence in large language models.",
        "Experiments": "- Learn V and C from trajectories:\n  - Data: Collect token-level hidden states h_t and next states h_{t+1} from Llama-2-7B and Mistral-7B on GSM8K (with CoT), AQuA-RAT, ANLI-R3, StrategyQA, TruthfulQA, plus synthetic contradiction trajectories (premise\u2192claim\u2192inverted claim). Label steps as coherent/incoherent using verified solvers (math checkers), NLI classifiers, and TruthfulQA veracity; include human spot-checks on 500 samples.\n  - Models: V(h) = MLP(lnorm(h)) \u2192 R_{\u22650}; C(h) = sigmoid(MLP(lnorm(h))). Train with pairwise margin loss: for coherent steps, enforce V(h_{t+1}) <= V(h_t) - m; for incoherent, V(h_{t+1}) >= V(h_t) + m. Train C with cross-entropy on safe/unsafe labels; add temporal smoothness penalty on V.\n  - Validation: AUROC for failure detection using V increase and C < 0; correlation between V slope and task failure; ablations vs entropy, log-prob, attention entropy, ||\u0394h||.\n- LBD decoding algorithm:\n  - Tier-1 (constrained lookahead): For each step, compute candidate next states h_{t+1}^{(i)} by simulating top-5 tokens (k=5, lookahead depth 1). Select the highest-log-prob token among those with C(h_{t+1}^{(i)}) >= 0 and minimal V(h_{t+1}^{(i)}), subject to a 95% likelihood floor.\n  - Tier-2 (minimal steering): If no candidate satisfies constraints, linearize next-state map around r_t (residual before MLP) with one backward pass to get Jacobian J. Solve a small quadratic objective: minimize ||\u03b4r||^2 s.t. V(h_{t+1} + J\u00b7\u03b4r) <= V(h_t) - \u03b1\u00b7V(h_t) and C(h_{t+1} + J\u00b7\u03b4r) >= 0. Implement via projected gradient with a norm cap; no parameter updates.\n  - Tier-3 (fallback): If constraints remain infeasible for two steps, locally resample n=5 candidates and majority-vote among those with highest C and lowest V.\n  - Metrics: Verified accuracy (GSM8K exact match with checker, AQuA acc), NLI accuracy and internal contradiction rate, TruthfulQA truthful score; per-step constraint satisfaction rate, fraction of steps with monotone V, latency overhead.\n- Baselines and ablations:\n  - Greedy, nucleus, typical decoding; DoLa/lookahead; PPLM/GeDi-style logit steering; perplexity and entropy-based gating; our prior energy-guided proxy (||\u0394h||^2 \u00d7 perplexity).\n  - Remove barrier (only Lyapunov), remove Lyapunov (only barrier), remove steering, vary k and likelihood floor, and replace V/C with output-space classifiers.\n- Transfer and robustness:\n  - Train V/C on Llama-2-7B, test on Mistral-7B (zero-shot transfer of evaluators); cross-domain tests (math\u2192NLI, NLI\u2192factuality). Evaluate AUROC shifts, constraint satisfaction, and performance deltas.\n  - Adversarial prompts injecting subtle contradictions or spurious chains; measure early-warning lead time (tokens between first constraint violation and final error).\n- Lightweight theory/checks:\n  - Provide discrete-time guarantee under linearization error \u03b5: projected update ensures one-step decrease and barrier satisfaction if feasible; empirically estimate \u03b5 bounds and feasibility rates.",
        "Risk Factors and Limitations": "- Proxy fidelity: Learned V and C may capture task difficulty rather than coherence; include partial correlations controlling for log-prob and length.\n- Calibration and transfer: V/C thresholds and scales may be model-specific; cross-model generalization could degrade.\n- Compute overhead: Lookahead and one backward pass per violation add latency; keep k small, cache Jacobians, and cap steering steps.\n- Feasibility: Linearization may be inaccurate in highly nonlocal regimes, leading to infeasible constraints; fallback must remain safe.\n- Access constraints: Requires hidden-state access and occasional gradients; incompatible with closed APIs.\n- Over-conservatism: Enforcing monotone V may dampen creativity or exploration; mitigate with likelihood floors and allow small non-monotone windows.\n- Evaluation noise: External verifiers/classifiers are imperfect; incorporate human audits and multiple validators to reduce label noise."
    },
    {
        "Name": "hodge_curl_decoding",
        "Title": "Hodge-Guided Decoding: Curl Minimization of Attention Flow for Coherent Language Model Reasoning",
        "Short Hypothesis": "In autoregressive transformers, incoherent steps are preceded by cyclic (curl-like) attention flow among tokens. A discrete Hodge decomposition of the attention graph separates conservative (gradient) from cyclic (curl) flow; spikes in curl energy anticipate contradictions, and locally minimizing predicted curl during decoding reduces incoherence without finetuning. This hidden-state graph setting is the right place to test the hypothesis: output-space uncertainty or entropy cannot distinguish benign exploration from pathological cyclic credit assignment, while attention graphs provide a natural flow field amenable to Helmholtz\u2013Hodge analysis.",
        "Related Work": "Attention rollout and flow analyses characterize how information passes through layers, but stop at visualization and attribution; they do not decompose attention into conservative vs cyclic components nor couple such structure to control. Uncertainty-based decoding (entropy, typicality), self-consistency, and output-space steering (PPLM/GeDi) operate on logits and tokens rather than internal state flows. Graph Hodge decomposition and simplicial methods are established in ML for analyzing edge flows, yet have not been applied to transformer attention for coherence monitoring or decoding. Our proposal is distinct in: (i) constructing per-step attention edge flows and performing a discrete Hodge decomposition to quantify cyclicity, (ii) using curl energy as an actionable early-warning signal for incoherence, and (iii) implementing curl-aware token selection and head gating to actively damp cyclic flow during inference.",
        "Abstract": "We propose Hodge-Guided Decoding (HGD), a simple, training-free control layer for autoregressive transformers that monitors and suppresses cyclic attention flow associated with incoherence. At each generation step, we form an attention edge-flow over the visible tokens by aggregating multihead attention across the last L layers and antisymmetrizing: F_ij = sum_{l,h}(A^{l,h}_{ij} - A^{l,h}_{ji}). We compute a discrete Hodge decomposition on this complete token graph, solving for node potentials \u03c6 that minimize \u2211_{i<j}(F_ij - (\u03c6_j - \u03c6_i))^2, and define the residual cycle flow R = F - \u2207\u03c6. The curl energy C = ||R||_F^2 (and triangle circulations) quantifies attention cyclicity. We show that C spikes and exhibits high coefficient-of-variation before self-contradictions and hallucinations. HGD acts when C exceeds an adaptive threshold: (1) curl-aware lookahead selects among top-k tokens the candidate that yields minimal predicted C_{t+1} subject to a likelihood floor, using a one-step forward simulation; and (2) a lightweight per-head gate downweights heads with highest cycle contribution, computed by attributing R to heads. Across math reasoning (GSM8K), NLI (ANLI-R3), and factuality (TruthfulQA), HGD improves verified accuracy and reduces contradiction rates over entropy/typical decoding and lookahead-only baselines with modest overhead. Unlike output-space methods, HGD targets a specific internal dynamical pathology\u2014cyclic attention flow\u2014providing an interpretable, physically motivated handle on coherence during decoding.",
        "Experiments": "- Constructing curl metrics and validation:\n  - States: Use Llama-2-7B and Mistral-7B. At each step t, aggregate attention over last L=4 layers and all heads on the current context window W (e.g., last 256 tokens). Define F_ij = \u2211_{l,h}(A^{l,h}_{ij} - A^{l,h}_{ji}). Solve least squares for potentials \u03c6 (closed-form via graph Laplacian) and compute R = F - (\u03c6_j - \u03c6_i). Metrics: curl energy C_t = ||R||_F^2; triangle circulation mean |R_ij + R_jk + R_ki| over sampled triples.\n  - Datasets: GSM8K (with CoT), AQuA-RAT, ANLI-R3, StrategyQA, TruthfulQA, and a synthetic contradiction set (premise\u2192claim\u2192inverted claim).\n  - Validation: Correlate {max C_t, CV(C)} with failure indicators (verified wrong answer, NLI contradiction, untruthful item). Report Spearman r and AUROC for failure detection using C_t vs baselines (token log-prob, entropy, attention entropy, ||\u0394h||). Measure early-warning lead time (#tokens between first C spike and failure).\n- Curl-aware lookahead (Tier-1):\n  - Algorithm: On steps with C_t above the 90th percentile (calibrated on held-out data), simulate next token for top-k=5 candidates (depth-1 lookahead). For each candidate, recompute attention on the extended sequence to estimate C_{t+1}. Choose the token with minimal C_{t+1} among those with log-prob \u2265 95% of the maximum.\n  - Evaluation: Verified task accuracy (exact match for GSM8K with a checker; accuracy for AQuA, ANLI, StrategyQA; TruthfulQA truthful score), internal contradiction rate (using an NLI classifier over generated sentences), latency overhead per token.\n- Head-wise curl attribution and gating (Tier-2):\n  - Attribution: Compute head contributions C^{l,h} = ||R^{l,h}||_F^2 where R^{l,h} is obtained by repeating the decomposition on per-head attention. When C_t is high, downweight the top-\u03b2=10% heads by a factor \u03b3\u2208[0.7,0.9] for the next step only.\n  - Ablations: Compare gating only, lookahead only, both, and none. Sweep \u03b2, \u03b3, L, and W. Evaluate accuracy/contradiction and generation diversity (distinct-n) to check for over-conservatism.\n- Robustness and transfer:\n  - Train thresholds on Llama-2-7B; test zero-shot on Mistral-7B. Evaluate AUROC shift, threshold recalibration cost, and performance deltas.\n  - Adversarial prompts injecting subtle contradictions; measure false-alarm rate on benign enumeration tasks to ensure C_t is not merely exploration-sensitive.\n- Efficiency checks:\n  - Implement Laplacian solve via a single Cholesky factorization per step on the W-window graph. Report ms/token overhead and memory. Compare to a fast approximation using only antisymmetric mass (F_ij>0) without solving for \u03c6.\n",
        "Risk Factors and Limitations": "- Proxy specificity: High curl energy may reflect task difficulty or coreference rather than incoherence; include partial correlations controlling for entropy and length, and analyze false positives (e.g., dialogue with backreferences).\n- Overhead: Lookahead requires k extra forward passes on spike steps; keep k small and bound trigger frequency. Head gating adds minimal cost but may reduce fluency if overused.\n- Applicability: Requires access to per-head attention maps; unavailable in some closed APIs or models with attention dropout/elision.\n- Incomplete guarantees: Curl damping has no formal correctness guarantee and targets local coherence; long-range global consistency may still fail.\n- Model variance: Thresholds for C_t may be model-specific; cross-model transfer can degrade without light recalibration.\n- Interaction with style: Aggressively minimizing curl may damp creative looping patterns (lists, dialogues); mitigate with likelihood floors and allow small tolerances around the threshold."
    },
    {
        "Name": "phase_lock_guided_decoding",
        "Title": "Phase-Locking Control: Kuramoto-Style Synchrony of Attention Dynamics for Coherent Transformer Reasoning",
        "Short Hypothesis": "Incoherent steps in autoregressive LMs are preceded by desynchronization among late-layer attention/head dynamics. A simple Kuramoto-style order parameter R_t, computed from instantaneous phases of low-dimensional projections of hidden/attention activity, reliably flags these instabilities. Enforcing mild phase re-synchronization during decoding\u2014via phase-aware token selection and lightweight per-head gating\u2014reduces contradictions with minimal overhead and without finetuning. Hidden-state phases are the right substrate: output-space entropy or perplexity cannot distinguish productive exploration from internal coordination breakdowns.",
        "Related Work": "Synchronization/phase-locking (e.g., Kuramoto models) is foundational in neuroscience and control but has not been applied to transformer hidden-state dynamics. Prior LM failure monitors emphasize output uncertainty (entropy, log-prob, SelfCheckGPT), self-consistency, or energy/invariant proxies; our earlier proposals considered energy, Lyapunov/barrier, or curl of attention flow. Mechanistic interpretability probes attention heads and residual geometry but does not define or control a phase-synchrony signal. Our contribution is new in (i) defining instantaneous phases for attention/residual dynamics via a 2D latent (PCA) or analytic signal, (ii) using a Kuramoto order parameter across heads/layers as an actionable coherence monitor, and (iii) decoding interventions that explicitly re-phase internal dynamics without altering LM weights.",
        "Abstract": "We propose Phase-Locking Control (PLC), an inference-time framework that treats the evolution of transformer internals as coupled oscillators. For the last L layers, we derive per-head (or per-layer) instantaneous phases by projecting residual-stream velocity \u0394h_t onto a 2D local principal subspace and computing \u03c6_t = atan2(\u27e8\u0394h_t, pc2\u27e9, \u27e8\u0394h_t, pc1\u27e9); similarly, we obtain head phases from attention-output vectors. A Kuramoto-style order parameter R_t = (1/H)|\u2211_h exp(i\u03c6_t^{(h)})| summarizes synchrony. We show that drops and high variance in R_t precede contradictions and hallucinations across math, NLI, and factuality tasks, while remaining stable during benign exploratory steps. PLC acts only on desync alarms: (1) phase-aware lookahead over top-k tokens selects the candidate that maximizes predicted R_{t+1} subject to a likelihood floor; and (2) a per-head phase gate scales head outputs by g_h = 1 \u2212 \u03b2\u00b7(1 \u2212 cos(\u03c6_t^{(h)} \u2212 \u03c6\u0304_t)), lightly favoring in-phase heads for one step. These controls require no finetuning or gradients and add modest overhead. On GSM8K (with CoT), ANLI-R3, StrategyQA, and TruthfulQA, PLC reduces internal contradiction rates and improves verified accuracy over strong uncertainty, lookahead, and output-steering baselines. Our results suggest that phase synchrony is a simple, interpretable substrate for online coherence control, opening a new bridge between synchronization theory and large language model alignment.",
        "Experiments": "- Signal construction and validation:\n  - Models: Llama-2-7B and Mistral-7B. Capture final-layer residuals and per-head attention outputs over a sliding window W=64 tokens. Compute \u0394h_t and a 2D PCA basis on \u0394h_{t\u2212W:t\u22121}; define \u03c6_t from 2D coordinates (also test Hilbert-transform phases on scalar head signals).\n  - Metrics: Kuramoto R_t across last L=4 layers\u2019 heads; features: min R, CV(R), negative slope events. Benchmarks: GSM8K (CoT), AQuA-RAT, ANLI-R3, StrategyQA, TruthfulQA, and a synthetic contradiction set.\n  - Analysis: Spearman correlation and AUROC of R-based alarms vs failures; partial correlations controlling for log-prob and attention entropy; compare against baselines (entropy, perplexity, ||\u0394h||, attention entropy).\n- Phase-aware decoding (Tier-1):\n  - On alarm steps (R_t below 10th percentile calibrated on held-out data), simulate depth-1 lookahead for top-k=5 tokens; recompute \u03c6 and R_{t+1}; choose token maximizing R_{t+1} with \u226595% of maximum log-prob.\n  - Evaluate verified accuracy (GSM8K exact-match with checker; AQuA acc; ANLI acc; TruthfulQA truthful score), internal contradiction rate (NLI classifier over self-referential sentences), and latency overhead.\n- Per-head phase gating (Tier-2):\n  - When alarms persist, apply g_h = 1 \u2212 \u03b2\u00b7(1 \u2212 cos(\u03c6_t^{(h)} \u2212 \u03c6\u0304_t)) (\u03b2\u2208[0.1,0.4]) to head outputs for the next token only. No parameter updates.\n  - Ablate \u03b2, L, W; compare gating-only, lookahead-only, both, and none.\n- Robustness and transfer:\n  - Train thresholds on Llama-2-7B; test zero-shot on Mistral-7B. Adversarial prompts inducing subtle contradictions. Measure early-warning lead time (tokens between first R drop and error) and false alarms on benign enumerations.\n- Phase estimator ablations:\n  - PCA-2D vs Hilbert (analytic signal) vs fixed 2D template learned on held-out runs. Compare stability, compute, and predictive power.",
        "Risk Factors and Limitations": "- Proxy fidelity: R_t may partly reflect task difficulty; use partial correlations and difficulty-matched controls.\n- Phase estimation noise: PCA bases may drift; mitigate with overlapping windows, exponential averaging, and basis-stability checks.\n- Overhead: Lookahead adds k extra forwards on alarm steps; keep k small and trigger sparsely.\n- Over-conservatism: Phase gating may damp diversity; maintain likelihood floors and revert gate quickly.\n- Access constraints: Requires per-head activations; not applicable to closed models without internals.\n- Scope: PLC targets local coordination; long-horizon global consistency still benefits from planning/verifiers.\n- Negative cases: Some coherent reasoning may involve deliberate desynchronization; include whitelists (e.g., list generation) and allow low-amplitude alarms in such regimes."
    },
    {
        "Name": "automaton_guarded_decoding",
        "Title": "Automaton-Guarded Decoding: Discrete State Quantization of Transformer Residuals for Coherent Reasoning and Self-Repair",
        "Short Hypothesis": "Coherent reasoning trajectories in LMs occupy a low-entropy, quasi-finite set of residual-stream states. By discretizing the final-layer residuals into a compact codebook and building a transition automaton from verified-correct runs, we can detect incoherence as violations of learned local determinism (rare or unseen state transitions). Using this automaton anomaly to gate token selection and apply small residual projections reduces contradictions more effectively than output-space uncertainty alone. The discrete hidden-state view is the right substrate: token-level entropy cannot capture internal transition violations, and we avoid heavy finetuning by learning a post-hoc, inference-time state quantizer.",
        "Related Work": "Codebook Features (Tamkin et al., ICML\u201923) introduce vector-quantized bottlenecks to make features discrete and controllable, but require model finetuning and focus on representation/control, not an explicit transition automaton for coherence monitoring. Automata extraction from RNNs (e.g., Weiss et al., 2024; classic Tomita-grammar studies) reconstruct DFAs from continuous states but aim at post-hoc analysis of sequence models, not online decoding control for transformers. Our proposal differs by: (i) post-hoc quantizing transformer residuals without finetuning; (ii) constructing a token-conditioned discrete state-transition model specifically from verified-correct trajectories; and (iii) using violations of local determinism as an actionable coherence signal to steer decoding and perform minimal state repairs at inference.",
        "Abstract": "We propose Automaton-Guarded Decoding (AGD), a simple, training-light framework that enforces coherence in frozen transformer LMs by treating their residual-stream dynamics as a discrete automaton. First, we collect final-layer residuals along generation trajectories and fit a compact codebook (k=256\u20131024) via k-means, mapping each step to a code z_t. From verified-correct runs on reasoning and factuality datasets, we estimate token-conditioned transitions T(z_t, token \u2192 z_{t+1}) and derive a determinism profile. At test time, AGD flags incoherence when the realized or predicted transition has high automaton anomaly: A_t = \u2212log ||T(z_t, token \u2192 z_{t+1})|| with Dirichlet smoothing, optionally augmented by per-state branching entropy. On alarm steps, AGD performs (i) automaton-aware lookahead: among top-k candidates, select the token minimizing predicted anomaly subject to a likelihood floor; and (ii) minimal residual projection: if alarms persist, add a small, norm-capped vector to the residual to snap toward the nearest low-anomaly code centroid for the next step, without updating LM weights. Across GSM8K, AQuA-RAT, ANLI, StrategyQA, and TruthfulQA, we evaluate AGD as a detector (AUROC of anomaly vs failure) and as a controller (verified accuracy, contradiction rate, compute overhead), comparing to entropy/perplexity gating and logit-space steering. AGD requires only hidden-state reads, a shallow codebook, and sparse lookahead, yet provides an interpretable, discrete-state guardrail that turns internal transition violations into concrete decoding corrections.",
        "Experiments": "- Build the discrete automaton:\n  - Data: Llama-2-7B and Mistral-7B. Collect final-layer residuals h_t on training splits of GSM8K (with a checker), AQuA, ANLI-R3, StrategyQA, and TruthfulQA. Keep only verified-correct trajectories to define the coherence automaton.\n  - Codebook: Fit k-means (k\u2208{256,512,1024}) on LayerNorm(h_t) to obtain centroids {c_i}. Map states via z_t = argmin_i ||h_t\u2212c_i||.\n  - Transitions: For each (z_t, token, z_{t+1}), accumulate counts; compute smoothed probabilities with \u03b1=1 Dirichlet prior. Per-state branching entropy H(z) = H(Z_{t+1}|z) from correct runs.\n- Signal validation and ablations:\n  - Anomaly: A_t = \u2212log p(z_{t+1}|z_t, token_t); optional combined score S_t = A_t + \u03b2\u00b7H(z_t), \u03b2\u2208[0,1].\n  - Metrics: AUROC/PR for failure detection (wrong math answer, NLI contradiction, untruthful item) using max A_t, CV(A), and S_t; Spearman correlation vs failure. Baselines: token entropy, log-prob, attention entropy, ||\u0394h||.\n  - Robustness: Partial correlations controlling for log-prob and length.\n- Automaton-aware decoding (Tier-1):\n  - On steps where predicted anomaly for the greedy token exceeds the 90th percentile (calibrated on held-out data), simulate depth-1 lookahead for top-k=5 tokens; compute z_{t+1}^{(i)} and A^{(i)}. Choose the token with minimal A^{(i)} subject to \u226595% of max log-prob.\n  - Evaluate: Verified accuracy (GSM8K exact match with checker; AQuA acc; ANLI acc; StrategyQA acc; TruthfulQA truthful score), internal contradiction rate (NLI-based on generated sentences), latency overhead.\n- Minimal residual projection (Tier-2):\n  - If two consecutive high-anomaly steps persist, project residual toward nearest safe centroid: r_t \u2190 r_t + \u03b7\u00b7(c_s \u2212 h_t), where s = argmin_i [H(i)+A\u0302(i)] over centroids reachable via high-prob transitions and \u03b7 is norm-capped (||\u03b7\u00b7(c_s\u2212h_t)||\u2264\u03b5).\n  - Evaluate: Additional gains vs Tier-1; sweep \u03b5 and selection criteria; compare to logit-space steering (e.g., PPLM-style) under equal compute.\n- Generalization and transfer:\n  - Train codebook/automaton on Llama-2-7B; test anomaly detection and control on Mistral-7B (zero-shot codebook application via nearest-centroid mapping). Measure AUROC shift and downstream gains; light recalibration if needed.\n- Efficiency and scale:\n  - Report ms/token overhead and memory; k and centroid size sweeps; windowed codebooks (last 128 tokens) vs global.\n",
        "Risk Factors and Limitations": "- Proxy specificity: High anomaly may reflect difficulty/novelty rather than incoherence. Mitigate via partial correlations, difficulty-matched subsets, and inclusion of benign exploration cases.\n- Coverage/sparsity: Transition counts can be sparse for large codebooks or rare tokens; use smoothing, back-off by marginalizing tokens, or reduce k.\n- Threshold calibration: Percentile thresholds may be model-specific; cross-model transfer could require light recalibration.\n- Access constraints: Requires hidden-state access and k small lookahead; closed APIs may be incompatible.\n- Over-conservatism: Favoring low-anomaly transitions may damp creativity; enforce likelihood floors and permit occasional high-anomaly steps.\n- Projection side effects: Residual projections may cause style shifts; keep updates small and transient.\n- Scope: Automaton guards local step consistency; global long-horizon planning still benefits from external verifiers/tools."
    },
    {
        "Name": "arrow_of_time_consistency",
        "Title": "Arrow-of-Time Consistency: Entropy-Production Proxies for Coherent Transformer Decoding",
        "Short Hypothesis": "Coherent reasoning trajectories in autoregressive transformers are near time-reversible at the level of residual-stream dynamics, while incoherence induces high irreversibility (entropy production). A small inverse-dynamics model that maps (h_{t+1}, token_t) -> h\u0302_t enables a forward\u2013reverse mismatch score that reliably anticipates contradictions and hallucinations. Using this Arrow-of-Time (AoT) score to gate lightweight, token-level lookahead reduces incoherence without modifying the base model. Hidden-state reversibility is the right substrate here: output-space uncertainty cannot measure dynamical irreversibility, and measuring \u0394h alone confounds difficulty with drift.",
        "Related Work": "Uncertainty-based monitors (log-prob, entropy, SelfCheckGPT) and self-consistency improve reliability but lack internal dynamical diagnostics. Output-space steering (PPLM, GeDi) controls attributes without monitoring state evolution. Our prior proposals (energy, Lyapunov/barrier, curl/phase) treat other physical analogies; none operationalize time-reversal asymmetry. Cycle-consistency is common in vision (e.g., unpaired translation) but has not been applied to transformer hidden-state trajectories, nor tied to entropy-production proxies for inference-time control. To our knowledge, learning an inverse dynamics over residual streams to estimate an Arrow-of-Time score and using it for decoding control is novel and not a trivial extension of entropy or attention-based methods.",
        "Abstract": "We propose Arrow-of-Time Consistency (AoT), a simple inference-time framework that detects and mitigates incoherent reasoning in large language models by measuring hidden-state irreversibility. We treat the final-layer residual stream as a discrete-time dynamical system and train a compact inverse-dynamics model g that predicts the previous residual from the next residual and emitted token: h\u0302_t = g(h_{t+1}, token_t). The AoT score is the forward\u2013reverse mismatch S_t = ||LN(h_t) \u2212 LN(h\u0302_t)||_2 or a learned Mahalanobis distance calibrated on correct trajectories. Intuitively, coherent reasoning remains near-reversible, while contradictions and hallucinations exhibit higher entropy production and thus larger S_t. AoT acts only on alarms: (i) AoT-aware lookahead simulates a one-step extension for the top-k tokens and selects the candidate minimizing predicted S_{t+1} under a likelihood floor; and (ii) if alarms persist, a cycle-consistency filter rejects candidates whose forward\u2192inverse loop deviates most from identity. AoT requires no base-model finetuning, gradients, or internal modification beyond reading hidden states and running a small predictor. Across math word problems, NLI, and factuality tasks, we show that S_t anticipates failures with strong AUROC, and AoT-guided decoding reduces contradiction rates and improves verified accuracy over entropy-, perplexity-, and lookahead-only baselines with modest overhead. Our results suggest that time-reversal asymmetry of hidden dynamics is an actionable substrate for coherence control, offering a thermodynamics-flavored lens distinct from uncertainty or attention-based heuristics.",
        "Experiments": "- Build the inverse dynamics and calibrate AoT:\n  - Data/models: Llama-2-7B and Mistral-7B. Collect final-layer residuals h_t on GSM8K (w/ CoT), AQuA-RAT, ANLI-R3, StrategyQA, TruthfulQA, and a synthetic contradiction set. Keep verified-correct trajectories (checker/NLI/veracity) for calibration.\n  - Inverse model: g(h_{t+1}, token_t) -> h\u0302_t. Architecture: 2-layer MLP on LayerNorm(h_{t+1}) concatenated with a learned token embedding; 512 hidden units; Gaussian head predicting mean and diagonal covariance. Loss: NLL of h_t under N(h\u0302_t, \u03a3\u0302_t) + temporal smoothness.\n  - AoT score: S_t = (LN(h_t)\u2212h\u0302_t)^T \u03a3\u0302_t^{-1} (LN(h_t)\u2212h\u0302_t) (falls back to L2 if \u03a3 unavailable).\n  - Validation: Correlate max S_t and CV(S) with failures; AUROC/PR for failure detection versus baselines (token log-prob, entropy, ||\u0394h||, attention entropy). Partial correlations controlling for log-prob and length; early-warning lead time.\n- AoT-aware decoding (Tier-1):\n  - Trigger: When S_t exceeds the 90th percentile (calibrated on held-out correct data), perform depth-1 lookahead over top-k=5 tokens. For candidate i, simulate next residual h_{t+1}^{(i)}, compute S_{t+1}^{(i)} using g. Choose the token minimizing S_{t+1}^{(i)} subject to \u226595% of max log-prob.\n  - Metrics: Verified accuracy (GSM8K exact-match via checker; AQuA acc; ANLI acc; StrategyQA acc; TruthfulQA truthful score), internal contradiction rate (NLI over self-referential sentences), and overhead (ms/token on alarm vs non-alarm steps).\n- Cycle-consistency filter (Tier-2):\n  - If consecutive alarms persist, for each top-k token compute loop deviation L^{(i)} = ||LN(h_t) \u2212 g(h_{t+1}^{(i)}, token_i)||_2. Reject candidates with L^{(i)} above the 75th percentile; pick best remaining by log-prob.\n  - Ablations: Tier-1 only, Tier-2 only, both; sweep k and likelihood floors.\n- Robustness and transfer:\n  - Train g on Llama-2-7B; test on Mistral-7B without retraining (nearest-token embedding init for unseen vocab if needed). Report AoT AUROC shift and downstream gains; light recalibration cost.\n  - Adversarial contradictions and benign enumerations: measure false alarms and specificity.\n- Comparisons and controls:\n  - Baselines: greedy/nucleus/typical decoding; lookahead without AoT; entropy or \u0394h gating; our prior energy proxy when available.\n  - Difficulty control: Stratify by token log-prob bins to test incremental predictive power of S_t.\n- Efficiency:\n  - Report inverse model FLOPs and memory; trigger frequency; amortize token embeddings and LayerNorm to keep overhead \u22641.5\u00d7 on alarm steps.",
        "Risk Factors and Limitations": "- Proxy fidelity: High AoT may reflect novelty/difficulty rather than incoherence; include partial correlations, difficulty-matched subsets, and qualitative audits.\n- Calibration drift: Percentile thresholds and \u03a3\u0302_t scale may be model-specific; cross-model transfer could require light recalibration or feature whitening.\n- Coverage: Inverse dynamics trained on correct runs may be less accurate off-manifold; guard with likelihood floors and avoid hard rejections early.\n- Overhead: Lookahead adds k extra forwards on alarm steps; keep k small and alarms sparse.\n- Access constraints: Requires hidden-state access; closed APIs may be incompatible.\n- Conservatism: Minimizing AoT may bias toward safer, less diverse tokens; mitigate with likelihood floors and occasional exploration.\n- Scope: AoT targets local irreversibility; very long-range consistency still benefits from external planning or verifiers."
    },
    {
        "Name": "thermodynamic_length_decoding",
        "Title": "Thermodynamic-Length Decoding: Fisher\u2013Rao Smooth Paths for Coherent Language Model Reasoning",
        "Short Hypothesis": "Self-contradictions and reasoning drift in autoregressive LMs are preceded by large, jagged steps of the next-token distribution across time. Measuring consecutive Fisher\u2013Rao (FR) distances on the probability simplex yields a simple, model-agnostic \"thermodynamic speed\" that anticipates incoherence. Gating decoding with a small lookahead to minimize the predicted FR step\u2014subject to a likelihood floor\u2014enforces smooth, low-dissipation trajectories and reduces contradictions more reliably than entropy, log-prob, or typicality alone. The probability-simplex geometry is the right setting: it directly assesses how the model\u2019s beliefs evolve, without needing hidden states or gradients.",
        "Related Work": "Typical sampling/decoding constrains outputs to the typical set via entropy-based criteria but does not control inter-step distributional motion. Uncertainty-based monitors (entropy, log-prob) and self-consistency assess confidence or aggregate agreement, not the geometry of belief updates. Contrastive/lookahead decoding optimizes likelihood-related proxies rather than path smoothness. Information geometry (Amari) and thermodynamic length characterize geodesics and dissipation in parametric families, but have not, to our knowledge, been instantiated for autoregressive decoding control in LMs. Our proposal differs by: (i) using Fisher\u2013Rao/Bhattacharyya geometry over next-token distributions to define a per-step speed/length; (ii) turning this into a real-time control objective via tiny lookahead; and (iii) demonstrating coherence gains without accessing hidden states or finetuning.",
        "Abstract": "We introduce Thermodynamic-Length Decoding (TLD), a training-free framework that improves coherence of frozen language models by treating generation as a path on the probability simplex. At each step t, we compute the Fisher\u2013Rao (FR) distance between consecutive next-token distributions, d_FR(p_{t-1}, p_t) = 2\u00b7arccos(\u2211_v sqrt(p_{t-1}(v) p_t(v)))\u2014equivalently the Bhattacharyya angle\u2014defining a thermodynamic speed s_t and cumulative length L = \u2211_t d_FR. We show that spikes in s_t and high CV(s) precede contradictions and hallucinations across reasoning and factuality tasks, while remaining low during stable chains-of-thought. TLD converts this diagnostic into action: on alarm steps, we perform a shallow lookahead over top-k tokens and select the candidate minimizing the predicted next-step FR distance d_FR(p_t, p_{t+1}^{(i)}) under a 95% likelihood floor; if needed, we add a tiny entropy-tilt to avoid degenerate flattening. This enforces smooth belief updates without touching hidden states or weights. Across GSM8K (with verified CoT), ANLI-R3, StrategyQA, and TruthfulQA, TLD reduces internal contradiction rate and improves verified accuracy over greedy, nucleus/typical decoding, and lookahead-only baselines, with <1.6\u00d7 overhead on alarm steps. Ablations show FR geometry\u2014not entropy or log-prob\u2014drives predictive power and control efficacy. Our results suggest that information-geometric path length is a simple, interpretable substrate for online coherence control, opening a lightweight alternative to hidden-state steering or finetuning for safer, more trustworthy LM reasoning.",
        "Experiments": "- Signal validation:\n  - Models/data: Llama-2-7B and Mistral-7B. Benchmarks: GSM8K (with CoT + checker), AQuA-RAT, ANLI-R3, StrategyQA, TruthfulQA, and a synthetic contradiction set (premise\u2192claim\u2192negated claim).\n  - Metrics: Per-step FR speed s_t = d_FR(p_{t-1}, p_t) using full vocab or a top-200 renormalization; summary stats: max s, mean s, CV(s), and cumulative length L. Evaluate correlation (Spearman) and AUROC/PR of {max s, CV(s), L} as failure detectors vs baselines: token entropy, log-prob, attention entropy (if available), KL(p_t||p_{t-1}).\n- TLD decoding (Tier-1):\n  - Trigger: When s_t exceeds the 90th percentile (calibrated on held-out correct runs).\n  - Intervention: Simulate depth-1 lookahead for top-k=5 tokens. For each candidate i, compute p_{t+1}^{(i)} and d_FR(p_t, p_{t+1}^{(i)}). Select the token minimizing FR distance subject to \u226595% of max log-prob; optional tiny entropy tilt \u03bb\u00b7H(p_{t+1}^{(i)}) with \u03bb\u2208[0,0.1].\n  - Evaluation: Verified accuracy (GSM8K exact match with checker; AQuA, ANLI, StrategyQA accuracy; TruthfulQA truthful score), internal contradiction rate (NLI classifier on generated statements), early-warning lead time (tokens between first s spike and failure), and ms/token overhead.\n- Lightweight predictor (Tier-2, optional):\n  - Train a 2-layer MLP to predict d_FR(p_t, p_{t+1}) from (p_t, candidate token embedding) to reduce lookahead cost. Compare predictor-guided vs explicit lookahead on accuracy and latency.\n- Ablations and controls:\n  - Replace FR with KL or L2 over logits; remove likelihood floor; vary k and the top-N used for FR (N\u2208{50,100,200,all}). Compare to greedy, nucleus, typical decoding, and lookahead-only.\n- Transfer/robustness:\n  - Thresholds calibrated on Llama-2-7B, evaluated zero-shot on Mistral-7B. Adversarial prompts with subtle contradictions and benign enumerations; measure false-alarm rates and threshold recalibration cost.",
        "Risk Factors and Limitations": "- Proxy specificity: High FR speed can reflect genuine topic shifts or difficulty, not incoherence. Mitigate via partial correlations controlling for entropy/log-prob and by allowing short, whitelisted high-speed bursts (e.g., list starts).\n- Approximation error: Computing FR on truncated top-N vocab introduces bias; report sensitivity and default to N\u2265100.\n- Overhead: Lookahead adds k extra forwards on alarm steps; keep k small and alarms sparse; optional predictor reduces cost.\n- Over-conservatism: Penalizing FR steps can bias toward conservative continuations; maintain likelihood floors and small entropy tilt.\n- Calibration/transfer: Percentile thresholds may be model-specific; light recalibration may be needed across models/domains.\n- Scope: TLD targets local belief smoothness; long-horizon global consistency still benefits from external verifiers or planning."
    },
    {
        "Name": "persistence_guided_decoding",
        "Title": "Persistence-Guided Decoding: Topological Signatures of Attention for Coherent Language Model Reasoning",
        "Short Hypothesis": "During coherent reasoning, the attention-induced token\u2013token graph maintains a stable topological signature (few long-lived cycles, predictable component mergers) under natural filtrations; incoherence is preceded by topological phase transitions detectable via persistent homology. A simple \"topological surprise\" score\u2014Wasserstein distance from a calibrated persistence-diagram template or spikes in persistent entropy/Betti curves\u2014reliably flags drift. Gating decoding with a tiny lookahead to minimize predicted topological surprise reduces contradictions without finetuning. Attention topology is the right substrate: output entropy/perplexity cannot capture structural changes in information flow, and this avoids heavy hidden-state steering.",
        "Related Work": "Topological data analysis (TDA) has been used to study deep networks (e.g., persistent homology of activations/weights) but is rarely applied online to guide inference, and we found no direct work applying persistent homology to transformer attention for decoding control (our searches for \"persistent homology attention transformer NLP\" returned no hits). Prior reliability efforts focus on output-space uncertainty (entropy, log-prob, SelfCheckGPT), self-consistency, or internal metrics like attention entropy, energy, or flow curl; none capture topological invariants. Mechanistic interpretability analyzes circuits/heads but does not compute or act on persistent homology of the evolving attention graph. Our proposal is distinct in (i) defining a per-step attention filtration and persistence-based instability metrics, (ii) learning a template of topological signatures from verified-coherent runs, and (iii) using topological surprise to drive lightweight lookahead and per-head gating during decoding.",
        "Abstract": "We propose Persistence-Guided Decoding (PGD), a training-free framework that improves coherence in frozen transformers by treating attention as a topological object. At each step, we build a weighted token graph from symmetrized attention aggregated over the last L layers on a sliding context window. We define a filtration either by edge-thresholding the attention weights or via a distance d_ij = 1 \u2212 W_ij and compute 0D/1D persistent homology of the induced Vietoris\u2013Rips (clique) complex using Gudhi/Ripser. From persistence diagrams we extract summary features\u2014persistent entropy, total H1 lifetime, counts of bars above a scale\u2014and compute a topological surprise score: (a) Wasserstein (or bottleneck) distance to a template diagram (barycenter) learned from verified-correct trajectories, or (b) z-scored spikes in these summaries relative to a held-out calibration. We show that topological surprise rises before self-contradictions and hallucinations, offering early-warning beyond entropy/log-prob. PGD acts only on alarms: (1) persistence-aware lookahead simulates a depth-1 extension for top-k tokens and selects the candidate minimizing predicted topological surprise subject to a 95% likelihood floor; and (2) a per-head gate downweights heads whose attention contributions maximally increase H1 lifetime or diagram distance for one step. Across math reasoning (GSM8K), NLI (ANLI), and factuality (TruthfulQA), PGD reduces internal contradiction rates and improves verified accuracy over entropy/typicality and lookahead-only baselines with modest overhead (window \u226464 tokens, k \u22645). Our results indicate that persistent homology provides a simple, interpretable, and actionable lens for online coherence control\u2014capturing structural instabilities in attention that precede failure and enabling corrective decoding without modifying model weights.",
        "Experiments": [
            "Construct attention filtrations and persistence metrics: Aggregate attention over last L=4 layers and all heads on a sliding window W=64 tokens. Build a symmetric weight matrix W (zero diagonal); define filtrations by (i) descending edge-weight thresholds or (ii) Vietoris\u2013Rips with distances d_ij=1\u2212W_ij. Compute H0/H1 persistence diagrams (PDs) per step with Gudhi/Ripser. Summaries: persistent entropy, total H1 lifetime TL1, count of long bars NH1(>\u03b5), Betti curves over thresholds.",
            "Calibration and detection: From verified-correct trajectories (math with checker, ANLI non-contradictions, TruthfulQA truthful items), compute a PD template (Wasserstein barycenter) and z-score distributions of summaries. Define topological surprise S_t as Wasserstein distance to the template, or a weighted sum of z-scored summaries. Evaluate failure detection: AUROC/PR of {max S_t, CV(S), TL1 spikes} vs baselines (token entropy, log-prob, attention entropy, ||\u0394h|| if available). Report Spearman correlations and early-warning lead time (tokens from first spike to failure).",
            "Persistence-aware decoding (Tier-1): On steps where S_t exceeds the 90th percentile (calibrated), simulate depth-1 lookahead for top-k=5 tokens; recompute attention on the extended sequence and PDs to estimate S_{t+1}^{(i)}. Choose the token minimizing S_{t+1}^{(i)} subject to \u226595% of max log-prob. Measure verified task accuracy (GSM8K exact match with checker; AQuA acc; ANLI acc; StrategyQA acc; TruthfulQA truthful), internal contradiction rate (NLI classifier over generated sentences), and overhead (ms/token when triggered).",
            "Head-wise topological gating (Tier-2): Attribute topological instability to heads by recomputing summaries after removing a head\u2019s contribution (W^(\u2212h)). When S_t is high, downweight the top-\u03b2=10% heads that most increase TL1 or diagram distance (scale by \u03b3\u2208[0.7,0.9]) for the next step only. Ablate \u03b2, \u03b3, W, L; compare gating-only, lookahead-only, both, and none.",
            "Ablations and alternatives: Replace Wasserstein with bottleneck distance; use only summary features (persistent entropy/TL1) without PDs; vary \u03b5 for long-bar counts. Compare to greedy, nucleus, typical, and lookahead-only baselines; and to simple graph statistics (modularity, clustering) to show topology-specific gains.",
            "Transfer and robustness: Calibrate on Llama-2-7B, test zero-shot on Mistral-7B. Evaluate AUROC shift and downstream gains; lightly recalibrate thresholds if needed. Test adversarial prompts with subtle contradictions and benign enumerations/lists to assess specificity. Report trigger rates and compute costs."
        ],
        "Risk Factors and Limitations": "- Computational overhead: Persistent homology on a 64-token window and H0/H1 is feasible but adds cost on alarm steps; keep k small and use fast libraries. If necessary, fall back to persistent entropy or Betti curves without full PDs.\n- Proxy fidelity: Topological spikes may reflect topic shifts/coreference rather than incoherence; mitigate via partial correlations controlling for entropy/log-prob and by whitelisting regimes (e.g., list starts) where spikes are benign.\n- Calibration/transfer: Template diagrams and thresholds may be model-specific; cross-model/domain transfer can require light recalibration.\n- Access constraints: Requires per-head attention maps; closed APIs may limit applicability.\n- Locality: PGD targets local structural instabilities; long-horizon global consistency still benefits from planning/verifiers.\n- Attribution granularity: Head-level gating uses approximate contributions; misattribution can damp useful heads\u2014mitigate with small, transient gates and likelihood floors."
    },
    {
        "Name": "susceptibility_guided_decoding",
        "Title": "Susceptibility-Guided Decoding: Perturbation Response of Hidden States as a Coherence Compass",
        "Short Hypothesis": "Locally incoherent steps in autoregressive LMs are marked by high susceptibility: tiny perturbations to late-layer residuals or attention produce large changes in the next-token distribution. A simple, inference-time susceptibility score\u2014estimated via small finite-difference perturbations to the final residual stream and micro-dropout variance\u2014reliably flags impending contradictions. Using this score to (i) choose among top-k tokens the continuation that minimizes predicted susceptibility and (ii) apply a one-step temperature smoothing when alarms persist reduces incoherence more effectively than perplexity/entropy alone. This hidden-state setting is the right place to test the hypothesis: output-space confidence cannot measure internal fragility, and we avoid heavy finetuning by using only read access and a few extra forward passes.",
        "Related Work": "Uncertainty proxies (log-probability, entropy, self-consistency) and Monte Carlo dropout/ensembles estimate predictive variance but operate in output space and are typically used for detection, not online control. Output-space steering methods (e.g., attribute control) modify logits without monitoring internal fragility. Mechanistic interpretability inspects attention/residual geometry but rarely quantifies local sensitivity as a control signal. Our proposal differs by: (i) defining a per-step susceptibility score from perturbation response of the final residual stream and micro-dropout variance in late layers; (ii) using this internal stability diagnostic to guide token selection at run time; and (iii) employing a minimal, training-free intervention (single-step temperature smoothing) when instability persists. To our knowledge, turning hidden-state perturbation sensitivity into a decoding objective for coherence is novel and not a trivial extension of entropy or standard MC-dropout usage.",
        "Abstract": "We introduce Susceptibility-Guided Decoding (SGD), a training-free framework that improves coherence of frozen language models by treating late-layer dynamics as a system whose local stability can be measured and controlled. At each token step t, SGD computes a susceptibility score S_t from two lightweight probes: (1) finite-difference sensitivity of the next-token distribution to small Gaussian perturbations \u03b4 added to the final-layer residual stream (r_t \u2192 r_t + \u03b4), summarized as the average KL divergence per unit noise; and (2) micro-dropout variance of logits from enabling a small dropout rate (e.g., 0.1) in the last L layers for a handful of stochastic passes. We show that spikes and high variance in S_t anticipate self-contradictions and hallucinations beyond what entropy/log-prob capture. SGD acts only when S_t exceeds an adaptive threshold: (i) susceptibility-aware lookahead simulates a depth-1 extension over the top-k tokens and selects the candidate minimizing predicted S_{t+1} subject to a likelihood floor; and (ii) if instability persists on consecutive steps, a one-step temperature smoothing (T>1 for that step only) damps sensitivity while respecting a minimum-likelihood constraint. No weights are updated, and only a few extra forward passes are needed on alarm steps. Across math reasoning, NLI, and factuality benchmarks, SGD reduces internal contradiction rates and improves verified accuracy over entropy/typical and lookahead-only baselines with modest overhead. Our results suggest that hidden-state perturbation response is an actionable, interpretable substrate for online coherence control, bridging uncertainty estimation and dynamical stability for safer LM reasoning.",
        "Experiments": "- Signal construction and validation:\n  - Models: Llama-2-7B and Mistral-7B (float16), run with internals accessible.\n  - Susceptibility score: For each step t, compute\n    \u2022 Finite-difference sensitivity J_t: sample m=3 perturbations \u03b4_i ~ N(0, \u03c3^2 I) applied to the final-layer residual r_t before the LM head; estimate J_t = mean_i [KL(p_t || p_t^{(\u03b4_i)})]/\u03c3^2 with \u03c3 set so that ||\u03b4||/||r_t|| \u2248 0.01.\n    \u2022 Micro-dropout variance V_t: enable dropout=0.1 in the last L=2 layers; run s=4 stochastic passes and take average per-token logit variance or Bhattacharyya angle variance between next-token distributions.\n    \u2022 Combine S_t = \u03b1\u00b7J_t + (1\u2212\u03b1)\u00b7V_t with \u03b1=0.5 (ablate).\n  - Data: GSM8K (with chain-of-thought and a checker), AQuA-RAT, ANLI-R3, StrategyQA, TruthfulQA, plus a synthetic contradiction set (premise\u2192claim\u2192negated claim).\n  - Validation metrics: Spearman correlation between {max S_t, CV(S)} and failures; AUROC/PR for failure detection vs baselines (token entropy, log-prob, attention entropy, ||\u0394h||); early-warning lead time (# tokens between first S spike and error). Partial correlations controlling for log-prob and sequence length.\n- Susceptibility-aware decoding (Tier-1):\n  - Trigger: When S_t exceeds the 90th percentile of a held-out calibration set.\n  - Intervention: Simulate depth-1 lookahead for top-k=5 candidates; estimate S_{t+1}^{(i)} with reduced probes (m=2, s=2). Choose the token minimizing S_{t+1}^{(i)} subject to log-prob \u2265 95% of the max.\n  - Evaluation: Verified task accuracy (GSM8K exact match with a checker; AQuA acc; ANLI acc; StrategyQA acc; TruthfulQA truthful score), internal contradiction rate (NLI-based), latency overhead (ms/token on alarm vs non-alarm steps).\n- Temperature smoothing fallback (Tier-2):\n  - If two consecutive alarms persist after Tier-1, apply a one-step temperature T \u2208 [1.05, 1.15] to logits before sampling/selection, chosen to minimize S_{t+1} under the same likelihood floor.\n  - Ablations: Tier-1 only vs Tier-1+Tier-2; sweep k, \u03b1, \u03c3, L, T; replace KL with FR/Bhattacharyya angle for J_t.\n- Comparisons and controls:\n  - Baselines: greedy, nucleus, typical decoding; lookahead-only (no S); entropy/entropy-slope gating; MC-dropout variance alone; finite-difference sensitivity alone.\n  - Efficiency: Report trigger rate and total overhead; cap probes per token to keep <1.6\u00d7 cost on alarm steps.\n- Transfer and robustness:\n  - Calibrate thresholds on Llama-2-7B; test zero-shot on Mistral-7B. Measure AUROC shift and performance deltas; light recalibration cost.\n  - Adversarial prompts with subtle contradictions and benign enumerations/lists; measure false-alarm rates and specificity.",
        "Risk Factors and Limitations": "- Proxy fidelity: High susceptibility may correlate with task novelty or topic shifts rather than incoherence; mitigate via partial correlations and difficulty-matched analyses.\n- Overhead: Extra forward passes for perturbations/dropout add latency; limit probes to alarm steps, keep m,s small, and cache intermediates.\n- Calibration: Percentile thresholds can be model-specific; cross-model transfer may need light recalibration.\n- Access constraints: Requires hidden-state access and enabling dropout at inference; closed APIs may be incompatible.\n- Conservatism: Minimizing susceptibility may bias toward safer, less diverse tokens; enforce likelihood floors and limit temperature smoothing to minimal values.\n- Local scope: The method targets local fragility; long-horizon global consistency may still require verifiers or external tools."
    },
    {
        "Name": "ricci_curvature_guided_decoding",
        "Title": "Curvature-Guided Decoding: Ricci Geometry of Attention Graphs for Coherent Language Model Reasoning",
        "Short Hypothesis": "Local incoherence in autoregressive LMs is preceded by negative discrete Ricci curvature in the attention-induced token graph, reflecting divergent information flow. A simple curvature score computed on a sparsified attention graph (using fast Forman-Ricci, optionally Ollivier-Ricci on small windows) reliably flags impending contradictions. Gating decoding to prefer candidates that increase predicted curvature, and lightly downweighting heads that contribute the most negative curvature, reduces incoherence more effectively than entropy/log-prob or attention entropy. Attention-graph curvature is the right substrate: it captures transport coupling and contraction/expansion of information flow that output-space uncertainty and prior flow/curl/topology signals do not, while remaining computationally feasible on sliding windows.",
        "Related Work": "Discrete Ricci curvature (Ollivier, Forman) has been applied to complex networks and, more recently, to graph ML for robustness and community detection, but we find no prior work applying Ricci-style curvature to transformer attention graphs or using it to control decoding (our targeted search for \"Ollivier-Ricci curvature attention transformer NLP decoding\" returned no papers). Prior reliability controls monitor output-space uncertainty (entropy, log-prob, self-consistency), or probe internals via different geometries (our earlier proposals used energy, curl/Hodge, persistent homology, phase synchrony). Curvature is distinct: it quantifies local contraction/divergence of transport on the attention graph, offering a complementary, interpretable scalar that is neither a trivial entropy proxy nor a mere flow statistic. Unlike output-space steering (PPLM/GeDi), our method enforces an internal geometric invariant without finetuning the base model.",
        "Abstract": "We propose Curvature-Guided Decoding (CGD), an inference-time framework that improves coherence in frozen transformers by measuring and acting on the Ricci geometry of attention. At each generation step, we construct a sparsified token graph over a sliding window by symmetrizing and thresholding aggregated multihead attention from the last L layers. We compute a discrete Ricci curvature for edges\u2014using fast Forman-Ricci by default and Ollivier-Ricci on small windows as a check\u2014and summarize a step-level curvature score K_t (e.g., mean edge curvature or fraction of edges below a negative threshold). We show that spikes of negative curvature and high curvature variance anticipate contradictions and hallucinations across math, NLI, and factuality tasks. CGD converts this diagnostic into control: on alarm steps, a one-step lookahead over top-k tokens selects the candidate that maximizes predicted curvature K_{t+1} subject to a likelihood floor; if instability persists, we apply head-wise gating that downweights the small set of heads whose contributions most reduce curvature, for one step only. No model weights are updated; we only read attention maps and run a few extra forward passes when triggered. Across GSM8K, ANLI, StrategyQA, and TruthfulQA, CGD reduces internal contradiction rates and improves verified accuracy over strong baselines (entropy/typicality and lookahead-only) with modest overhead. Our results suggest that discrete Ricci curvature provides a simple, interpretable, and actionable handle on information-flow contraction in transformers, offering a new geometric substrate for online coherence control distinct from uncertainty, curl, or topology-based signals.",
        "Experiments": "- Curvature signal construction and validation:\n  - Models/data: Llama-2-7B and Mistral-7B. Benchmarks: GSM8K (with CoT + checker), AQuA-RAT, ANLI-R3, StrategyQA, TruthfulQA, plus a synthetic contradiction set (premise\u2192claim\u2192negated claim).\n  - Attention graph: For a sliding window W=64 tokens, aggregate attention over last L=4 layers; symmetrize W_ij = (A_ij + A_ji)/2. Sparsify by keeping each node\u2019s top-s=5 edges and renormalize.\n  - Curvature: Compute Forman-Ricci curvature F(e) on edges via the standard combinatorial formula for weighted graphs; define K_t = mean_e F(e) and NegFrac_t = fraction of edges with F(e) < -\u03c4 (\u03c4=0.1). On a held-out subset, also compute Ollivier-Ricci on the sparsified graph to confirm trends.\n  - Validation: Correlate {min K_t, max NegFrac_t, CV(K)} with failures (wrong math answer by checker, NLI contradiction, untruthful item). Report Spearman r, AUROC/PR for failure detection versus baselines (token entropy, log-prob, attention entropy, ||\u0394h||). Measure early-warning lead time (tokens from first curvature alarm to failure). Control for difficulty via partial correlations with log-prob.\n- Curvature-aware decoding (Tier-1):\n  - Trigger: When K_t falls below the 10th percentile or NegFrac_t exceeds the 90th percentile (calibrated on held-out correct runs).\n  - Intervention: Simulate depth-1 lookahead for top-k=5 candidates. For each candidate, recompute the attention graph on the extended sequence (same sparsification) and estimate K_{t+1}^{(i)} and NegFrac_{t+1}^{(i)}. Select the token maximizing K_{t+1}^{(i)} (or minimizing NegFrac) subject to \u226595% of the max log-prob.\n  - Metrics: Verified accuracy (GSM8K exact match via checker; AQuA acc; ANLI acc; StrategyQA acc; TruthfulQA truthful score), internal contradiction rate (NLI classifier over generated statements), alarm trigger rate, and ms/token overhead when triggered.\n- Head-wise curvature attribution and gating (Tier-2):\n  - Attribution: Recompute K_t per head by forming W^h from head h alone; rank heads by their contribution to negative curvature (decrease in K_t when adding head h). When alarms persist for two steps, downweight the top-\u03b2=10% heads by a factor \u03b3 in [0.7,0.9] for the next token only.\n  - Ablations: Compare gating-only, lookahead-only, both, and none; sweep \u03b2, \u03b3, L, W, and sparsity s.\n- Comparisons and controls:\n  - Baselines: greedy, nucleus, typical decoding; lookahead-only (no curvature); entropy/entropy-slope gating; attention entropy; our earlier curl/Hodge and persistence metrics (re-implemented) to demonstrate curvature\u2019s distinct signal.\n  - Replace Forman with Ollivier on small windows (W=32) to test robustness; replace curvature with simple graph stats (modularity, clustering) to show curvature-specific gains.\n- Transfer and robustness:\n  - Calibrate thresholds on Llama-2-7B; evaluate zero-shot on Mistral-7B. Report AUROC shift and downstream gains; light recalibration cost if needed.\n  - Adversarial prompts with subtle contradictions; benign enumerations/lists to assess false-alarm specificity.\n- Efficiency:\n  - Profile curvature computation (O(W\u00b7s) edges) and lookahead overhead; ensure <1.5\u00d7 cost on alarm steps and <5% overall slowdown with typical trigger rates.",
        "Risk Factors and Limitations": "- Proxy specificity: Negative curvature can reflect topic shifts or coreference patterns rather than incoherence. Mitigate with partial correlations controlling for entropy/log-prob and by whitelisting regimes (e.g., list starts) where drops are benign.\n- Overhead: Lookahead and curvature computation add cost on alarm steps; keep graphs sparsified (top-s edges), k small, and L, W modest.\n- Calibration/transfer: Percentile thresholds may be model-specific; cross-model/domain transfer could require light recalibration.\n- Access constraints: Requires per-head attention maps; closed APIs or attention-less variants limit applicability.\n- Attribution noise: Per-head curvature contributions are approximate; keep gates small and transient to avoid harming useful heads.\n- Local scope: Curvature guards local information-flow contraction; long-horizon consistency still benefits from planning or external verifiers."
    },
    {
        "Name": "peclet_guided_decoding",
        "Title": "Peclet-Guided Decoding: Advection\u2013Diffusion Attention Dynamics for Coherent Language Model Reasoning",
        "Short Hypothesis": "During coherent reasoning, the last-token attention behaves like directed transport with limited spread, while incoherence is preceded by either overly diffusive reading (wandering) or overly ballistic jumps (tunnel vision). A simple advection\u2013diffusion ratio, a Peclet-like number computed from the last-token attention over the context window, reliably flags these regimes. Constraining decoding to keep this Peclet number within a calibrated corridor via tiny lookahead and lightweight head gating reduces contradictions without finetuning. This hidden-attention transport view is the right substrate: output-space entropy cannot distinguish directional transport from diffusion, and it requires only read access to attention.",
        "Related Work": "Prior reliability work largely operates in output space: entropy/log-prob thresholds, typical sampling, self-consistency, and verifier reranking. Mechanistic interpretability visualizes attention flow and computes rollout, but does not cast attention as a transport process with advection and diffusion nor use that structure for control. We find no work using Peclet-like ratios on attention to guide decoding (our targeted search for \"transformer attention advection diffusion Peclet\" yielded no papers). Unlike attention entropy, which measures spread alone, our approach explicitly separates direction (advection) and spread (diffusion), and turns their ratio into an actionable constraint for token selection and per-head gating. It differs from prior geometric controls (curl/Hodge, curvature, phase-locking) by focusing on first- and second-moment transport statistics rather than cycle structure or synchrony.",
        "Abstract": "We propose Peclet-Guided Decoding (PGD), a training-free control layer for autoregressive transformers that treats last-step attention as an advection\u2013diffusion process. At each generation step t, we aggregate multihead attention from the final L layers for the last token over a sliding context window of size W. We compute two simple statistics over positions j: a directed advection length mu_t (the attention-weighted displacement from the last position) and a diffusion scale sigma_t (the attention-weighted standard deviation). Their ratio Pe_t = |mu_t| / (sigma_t + eps) serves as a Peclet-like number summarizing transport versus spread. We calibrate a corridor [Pe_min, Pe_max] on verified-correct trajectories; during incoherence, Pe_t tends to fall below (diffusive wandering) or exceed (ballistic tunnel vision) this corridor. PGD acts only on alarms when Pe_t is outside the corridor: (1) corridor-aware lookahead that evaluates top-k candidate tokens with a one-step simulation and picks the candidate minimizing the squared deviation of Pe_{t+1} from the corridor, subject to a likelihood floor; and (2) a lightweight per-head gate that temporarily downweights heads with extreme per-head Pe_h that drive the aggregate Pe_t away from the corridor. No weights are updated; the method requires only attention reads and a few extra forward passes on alarm steps. Across math word problems (GSM8K), NLI (ANLI-R3), and factuality (TruthfulQA), we evaluate PGD as a detector (AUROC of Pe-based alarms vs failures) and as a controller (verified accuracy and contradiction rate). We find that Peclet-like transport ratios provide an interpretable, actionable signal that improves coherence over entropy/typicality and lookahead-only baselines with modest overhead.",
        "Experiments": "- Construct the Peclet signal and validate predictive power:\n  - Models/data: Llama-2-7B and Mistral-7B with attention accessible. Benchmarks: GSM8K with CoT and a checker, AQuA-RAT, ANLI-R3, StrategyQA, TruthfulQA, plus a synthetic contradiction set (premise -> claim -> negated claim).\n  - Signal: Aggregate last L=4 layers' attention for the final token over a sliding window of size W=128. Let w_j be the normalized attention weight on position j. Compute mu_t = sum_j w_j * (t - j) and sigma_t^2 = sum_j w_j * ((t - j) - mu_t)^2; define Pe_t = |mu_t| / (sigma_t + 1e-6). Also compute per-head Pe_h using head-specific weights.\n  - Validation: Correlate {min Pe_t, max deviation from corridor, CV(Pe)} with failures (wrong math by checker, NLI contradiction, untruthful item). Report Spearman r and AUROC/PR for failure detection versus baselines: token entropy, log-prob, attention entropy, and ||delta h|| if available. Partial correlations controlling for log-prob and length; report early-warning lead time (tokens from first alarm to failure).\n- Corridor calibration:\n  - From verified-correct trajectories, estimate a corridor [Pe_min, Pe_max] as the central 60\u201380% quantile of Pe_t values. Analyze per-domain corridors (math vs NLI vs factuality) and choose a unified or domain-specific corridor via held-out validation.\n- Peclet-aware decoding (Tier-1):\n  - Trigger: If Pe_t is outside the corridor.\n  - Intervention: Simulate depth-1 lookahead for top-k=5 tokens; recompute Pe_{t+1}^{(i)} for each candidate on the extended sequence. Choose the token minimizing squared deviation from corridor (0 if inside) subject to log-prob >= 95% of the maximal candidate.\n  - Metrics: Verified accuracy (GSM8K exact match, AQuA acc, ANLI acc, StrategyQA acc, TruthfulQA truthful score), internal contradiction rate (NLI over generated statements), alarm trigger rate, and added ms/token on alarm vs non-alarm steps.\n- Per-head Peclet gating (Tier-2):\n  - When alarms persist for two steps, compute per-head Pe_h from head-specific attention. Downweight the top-10% heads that push aggregate Pe_t furthest outside the corridor by a factor gamma in [0.7, 0.9] for the next step only; recompute candidate selection as in Tier-1.\n  - Ablations: Gating-only, lookahead-only, both, and none; sweep gamma, L, W, and k. Compare against attention-entropy gating under equal compute.\n- Robustness and transfer:\n  - Calibrate the corridor on Llama-2-7B; evaluate zero-shot on Mistral-7B. Measure AUROC shift, performance deltas, and light recalibration costs.\n  - Adversarial prompts inducing subtle contradictions; benign enumerations/lists to assess false alarms. Whitelist regimes (e.g., list starts) where transient low or high Pe is benign.\n- Efficiency and alternatives:\n  - Profile overhead; ensure typical trigger rates keep total slowdown <5%. Test a lightweight predictor (2-layer MLP) that estimates Pe_{t+1} from current attention stats and candidate token embedding to reduce lookahead cost; compare latency vs accuracy.",
        "Risk Factors and Limitations": "- Proxy specificity: Extreme Pe may reflect topic shifts or list formatting rather than incoherence. Use partial correlations, whitelists for known benign regimes, and analyze false positives.\n- Corridor calibration: Corridors may be model- or domain-specific; cross-model transfer could require light recalibration.\n- Overhead: Lookahead adds k extra forwards on alarm steps; keep k small and trigger sparsely. Head gating is cheap but must be conservative.\n- Access constraints: Requires per-head attention maps; closed APIs or models without accessible attention limit applicability.\n- Over-conservatism: Favoring corridor compliance may damp creativity; enforce likelihood floors and allow occasional, bounded excursions.\n- Local scope: The method guards local reading dynamics; long-horizon global consistency may still need external verifiers or planning."
    },
    {
        "Name": "martingale_guardrails",
        "Title": "Martingale Guardrails: Nonparametric E-Values for Online Coherence Detection and Repair in Language Models",
        "Short Hypothesis": "There exist simple, low-dimensional nonconformity features of transformer decoding steps whose distributions are stable during coherent reasoning. By converting these features into per-step e-values and multiplying them into a nonnegative test martingale, we can detect on-the-fly deviations (reasoning drift, contradictions) with finite-time error control (via Ville\u2019s inequality). Triggering tiny, likelihood-constrained interventions only when the martingale crosses calibrated thresholds reduces incoherence more reliably than ad\u2011hoc entropy or perplexity thresholds. Decoding time is the right setting: sequential testing and optional-stopping guarantees fundamentally require an online process; offline or output-only methods cannot provide per-step error-controlled alarms.",
        "Related Work": "Uncertainty methods (entropy, log-prob, self-consistency/ensembles) monitor outputs but lack sequential error control. Conformal prediction has been explored for calibration in NLP, yet not instantiated as a token-by-token martingale that drives decoding decisions. Prior work on hallucination detection (e.g., SelfCheckGPT) aggregates post-hoc agreement rather than providing e-value supermartingales with optional-stopping guarantees. Our proposal is distinct in (i) defining lightweight, internal and output-side nonconformity features at each token, (ii) mapping them to e-values that form a nonnegative test martingale under a coherency null learned from verified-correct runs, and (iii) using threshold crossings to gate minimal, actionable repairs. It does not rely on geometry/energy/topology or gradient-based steering and is not a trivial extension of entropy thresholds or standard conformal calibration.",
        "Abstract": "We propose Martingale Guardrails, an inference-time framework that detects and repairs incoherent reasoning in frozen language models using nonparametric test martingales. At each token step, we compute a small set of nonconformity features capturing local stability and predictability\u2014e.g., surprisal slope, logit dispersion change, and residual-stream motion magnitude (if internals are available). From verified-correct trajectories we learn a null calibration to map each feature vector to a per-step e-value; their product forms a nonnegative martingale that, by Ville\u2019s inequality, controls the probability of false alarms at any optional stopping time. When the martingale crosses a calibrated boundary, we trigger tiny, likelihood-constrained interventions: depth\u20111 lookahead that prefers candidates with minimal predicted e-value, and a micro-backtrack that reconsiders the last decision if alarms persist. No base-model parameters are updated; the approach requires only feature reads and a few extra forward passes on alarm steps. Across math reasoning (GSM8K), NLI (ANLI-R3), and factuality (TruthfulQA), we evaluate the martingale both as a detector (AUROC, time\u2011to\u2011alarm) and as a controller (verified accuracy, contradiction rate, overhead), comparing to entropy/log-prob gates, self-consistency, and lookahead-only baselines. Results indicate that e-value martingales provide precise, early warnings that translate into measurable coherence improvements with modest compute. We argue that sequential, error\u2011controlled alarms offer a principled substrate for active coherence preservation, complementing existing uncertainty and interpretability tools.",
        "Experiments": "- Feature design and calibration:\n  - Models: Llama-2-7B and Mistral-7B (float16). Access logits; optionally access final-layer residuals for an internal variant.\n  - Features per step t: (i) surprisal increment s1 = max(0, \u2212log p_t(x_t) + log p_{t-1}(x_{t-1})), (ii) logit dispersion change s2 = |H(p_t) \u2212 H(p_{t-1})|, (iii) distributional motion s3 = Bhattacharyya angle between p_{t-1} and p_t; optional internal s4 = ||\u0394h_t||/||h_{t-1}||.\n  - Null calibration: Collect features on verified-correct trajectories (GSM8K with checker; ANLI non-contradictions; TruthfulQA truthful items). Fit a lightweight score-to-e mapping: (a) obtain per-feature conformal p-values via prequential calibration on a sliding window; (b) convert p-values to e-values with a safe e-transform w(u)=c\u00b7u^{c\u22121} (c\u2208(0,1)), and combine features multiplicatively or via a small calibrated mixture e_t = \u03a3_j \u03b1_j w(p_{t}^{(j)}), \u03b1_j\u22650, \u03a3\u03b1_j=1. The product E_t=\u220f_{\u03c4\u2264t} e_\u03c4 is our test martingale.\n  - Validation: AUROC/PR of e_t and log E_t for detecting failures; time-to-alarm (tokens between first boundary crossing and verified error). Compare against entropy/log-prob thresholds and FR/KL step size.\n- Martingale thresholds and guarantees:\n  - Set a global alarm boundary B=1/\u03b4 (e.g., \u03b4=0.05) so P(null ever triggers)\u2264\u03b4 by Ville\u2019s inequality. Optionally use spending schedules (mixture boundaries) to trade sensitivity vs early false alarms.\n- Guardrail decoding (Tier-1: lookahead):\n  - Trigger: When E_t\u2265B\u2019 (a softer pre-alarm, e.g., B\u2019=B/3), evaluate top-k=5 candidates with depth-1 simulation to estimate e_{t+1}^{(i)}. Choose the token minimizing e_{t+1}^{(i)} subject to \u226595% of max log-prob.\n  - Metrics: Verified accuracy (GSM8K exact match with checker; ANLI accuracy; TruthfulQA truthful score), internal contradiction rate (NLI-based), overhead on alarm vs non-alarm steps.\n- Guardrail decoding (Tier-2: micro-backtrack):\n  - If E_t\u2265B, backtrack one token to t\u22121 and select the next-best token under the same likelihood floor but with minimal predicted e_t. Resume decoding; cap to one backtrack per 50 tokens.\n  - Ablations: Tier-1 only vs Tier-1+Tier-2; vary k and likelihood floors.\n- Comparisons and controls:\n  - Baselines: greedy, nucleus, typical decoding; entropy thresholding; lookahead-only; self-consistency/Reflexion reranking. If internals available, compare to a simple ||\u0394h|| threshold.\n  - Difficulty control: Stratify by token log-prob bins to test incremental predictive power of e-values.\n- Transfer and robustness:\n  - Calibrate on Llama-2-7B; evaluate zero-shot on Mistral-7B and vice versa. Report AUROC shift, threshold recalibration cost, and downstream gains.\n  - Adversarial prompts inducing subtle contradictions; benign enumerations/lists to assess false-alarm specificity.\n- Efficiency:\n  - Report trigger rates and total slowdown; keep overall overhead <5% by limiting lookahead to alarm steps and k\u22645. Provide a no-internals variant (s1\u2013s3 only).",
        "Risk Factors and Limitations": "- Null misspecification: If the calibrated null captures task difficulty rather than coherence, alarms may overfire; mitigate via verified-correct data, partial correlations controlling for log-prob, and per-domain calibration.\n- Exchangeability violations: Sequential dependence can weaken nominal guarantees; prequential calibration and sliding windows help, but guarantees remain approximate in practice.\n- Overhead: Lookahead adds k extra forwards on alarm steps; keep k small and alarms sparse. Micro-backtracking adds limited recomputation.\n- Transfer drift: Thresholds and e-transforms may be model/domain specific; include light recalibration for new models.\n- Conservatism: Guardrails may bias toward safer tokens; maintain likelihood floors and permit occasional high e_t if downstream verifiers approve.\n- Access constraints: Internal feature s4 requires hidden states; provide an output-only variant. Scope remains local; long-horizon global consistency still benefits from external tools/verifiers."
    },
    {
        "Name": "doob_transformed_decoding",
        "Title": "Doob-Transformed Decoding: Committor-Guided Control of Transformer Reasoning",
        "Short Hypothesis": "There exists a compact committor function q(h_t) mapping transformer residual states to the probability of eventual coherent success (e.g., verified-correct answer or non-contradiction). Using an approximate Doob h-transform at inference\u2014biasing next-token choices toward increasing q\u2014reduces contradictions more reliably than output uncertainty or lookahead alone. Autoregressive decoding is the right setting: the committor explicitly conditions on reaching success before failure over the remaining trajectory, which simpler token-level proxies cannot capture.",
        "Related Work": "Minimum Bayes Risk decoding and self-consistency optimize output-space criteria but do not model the conditional probability of eventual success from internal states. RLHF/reward-guided decoding steers via preference models over text, not a survival/committor probability over hidden states; GeDi/attribute models are class-conditional and operate on logits, not on outcome-conditioned trajectory probabilities. Safe control analogs (Lyapunov/barrier) enforce invariants, whereas our approach is probabilistic and goal-conditioned via the Doob transform. We found no prior work learning a committor over transformer residuals and using it to perform a Doob-style tilt during decoding; survival/hazard modeling for LMs has focused on token timing or length, not coherence outcomes.",
        "Abstract": "We propose Doob-Transformed Decoding (DTD), a simple inference-time control method that reduces incoherence in frozen language models by learning and exploiting a committor function over hidden states. The committor q(h_t) estimates the probability that, from the current residual state h_t, generation will reach a coherent success (verified-correct answer or non-contradictory completion) before failure. We train a compact predictor q on trajectories labeled by outcome using only hidden-state reads, with optional calibration. At test time, DTD performs a lightweight Doob-style reweighting: for the top-k candidate tokens, we simulate one-step next states h_{t+1}^{(i)} and adjust selection by a committor tilt log p(x_{t+1}=i) + \u03b2\u00b7log q(h_{t+1}^{(i)}) (or by maximizing \u0394 log q), subject to a likelihood floor. A hazard head \u03bb(h_t) provides an alarm signal when instantaneous failure risk spikes, gating when to invoke lookahead. DTD requires no finetuning of the base LM, no gradients, and only a few extra forward passes on alarmed steps. Across math reasoning (GSM8K with programmatic verification), NLI (ANLI), and factuality (TruthfulQA), we evaluate q as a detector (calibration, AUROC vs failures) and DTD as a controller (verified accuracy, contradiction rate, overhead), comparing to entropy/typical decoding, lookahead-only, MBR-style reranking, and attribute/reward-steered baselines. We find that a low-capacity committor over residuals is well-calibrated and predictive, and the Doob tilt translates these predictions into consistent coherence gains with modest compute. The results suggest outcome-conditioned hidden-state probabilities are a practical substrate for active coherence preservation in large language models.",
        "Experiments": [
            "Train a committor and hazard over residuals: Collect final-layer residuals h_t and outcomes on Llama-2-7B and Mistral-7B for GSM8K (with checker), AQuA-RAT, ANLI-R3, StrategyQA, TruthfulQA, plus a synthetic contradiction set. Train q(h_t) = MLP(LN(h_t)) -> [0,1] with cross-entropy to predict eventual success; add isotonic or Platt calibration. Train a hazard head \u03bb(h_t) via logistic regression/MLP to predict failure-at-next-step labels (using verifier/NLI), providing an alarm trigger.",
            "Validate committor as a detector: Report AUROC/PR for q(h_t) predicting sequence-level success from mid-trajectory states; calibration metrics (ECE, reliability diagrams); early-warning lead time (tokens between low-q alarms and eventual failure). Compare to baselines: token log-prob, entropy, ||\u0394h||, and an output-only committor using p_t features (no hidden states).",
            "Doob-Transformed Decoding (Tier-1): On steps where \u03bb(h_t) or a drop in q exceeds a calibrated percentile (e.g., top 10%), perform depth-1 lookahead over top-k=5 tokens. For each candidate, compute h_{t+1}^{(i)} and q_{t+1}^{(i)}. Select argmax over log p(i) + \u03b2\u00b7log q_{t+1}^{(i)} with a 95% likelihood floor; ablate \u03b2 and \u0394 log q variants. Evaluate verified accuracy (GSM8K exact match with checker; AQuA, ANLI, StrategyQA accuracy; TruthfulQA truthful score), internal contradiction rate (NLI-based), and overhead.",
            "Beam/MBR ablations (Tier-1b): Replace greedy selection with a small beam (width 3\u20135) using committor-prior scoring: sum of token log-probs + \u03b2\u00b7sum_t log q(h_t). Compare to standard beam and MBR reranking of samples using verifier proxies.",
            "Fallback micro-resample (Tier-2): If two consecutive high-hazard/high-drop steps persist, locally resample n=5 candidates and select the one with highest q subject to likelihood floor. Compare to lookahead-only and to reward/attribute-steered decoding (GeDi-like) under equal compute.",
            "Transfer and robustness: Train q, \u03bb on Llama-2-7B; test zero-shot on Mistral-7B (map LN scales if needed). Report AUROC shift and downstream gains; light recalibration cost. Adversarial prompts inducing subtle contradictions and benign enumerations to assess specificity and false alarms.",
            "Ablations and controls: Hidden-state q vs output-only q; effect of calibration; vary k and alarm thresholds; compare to entropy/entropy-slope gating, lookahead-only, MBR, and RLHF reward-guided decoding if a public reward model is available. Efficiency profiling: trigger rate and ms/token; target <1.6\u00d7 overhead on alarm steps and <5% overall slowdown."
        ],
        "Risk Factors and Limitations": "- Proxy attribution: q may correlate with task difficulty rather than coherence; mitigate via partial correlations controlling for log-prob, difficulty-stratified analyses, and verifier-grounded labels.\n- Calibration and transfer: q and \u03bb scales can be model-specific; cross-model transfer may require light recalibration (e.g., temperature on LN features).\n- Compute overhead: Lookahead adds k extra forwards on alarm steps; keep k small and alarms sparse.\n- Label noise: Verifiers (math checkers/NLI) are imperfect; include human audits on a subset and aggregate multiple signals.\n- Conservatism: Doob tilt may bias toward safe continuations; enforce likelihood floors and bound \u03b2 to avoid mode collapse.\n- Access constraints: Requires hidden-state reads; provide an output-only variant with weaker but still actionable improvements.\n- Scope: DTD targets local stepwise improvement in success probability; very long-range consistency still benefits from external planning/verifiers."
    },
    {
        "Name": "innovation_observer_decoding",
        "Title": "Innovation-Observer Decoding: Luenberger-Style Hidden-State Monitoring and Correction for Coherent LM Reasoning",
        "Short Hypothesis": "A compact hidden-state observer that predicts next-step residuals/logits in a frozen transformer yields a whitened innovation signal whose magnitude and serial correlations spike specifically at the onset of incoherence. Using this innovation to gate tiny, likelihood-constrained lookahead and to apply a minimal observer-style correction to the residual stream reduces contradictions and hallucinations more reliably than output-space entropy or perplexity alone, without finetuning or backprop at inference. Autoregressive decoding is the right setting: discrete-time steps and accessible residuals/logits enable a formal observer with innovation-whiteness tests; simpler output metrics cannot assess internal model\u2013observer mismatch.",
        "Related Work": "Predictive-coding and Kalman/Luenberger observers are classical in control and have recent neural variants (e.g., neural Kalman filters, predictive-coding networks), but we found no work applying an innovation-based observer to transformer hidden-state trajectories for decoding control (our targeted searches for Kalman/Luenberger observer + transformer decoding returned no hits). Prior LM reliability approaches emphasize output-space uncertainty (log-prob, entropy, SelfCheckGPT), self-consistency/reranking, or hidden-state proxies we proposed earlier (energy, Lyapunov/barrier, curl/topology, AoT). The closest is a generic self-predictor of internals; we differ by: (i) fitting an explicit observer with calibrated innovation covariance; (ii) using whitened innovation magnitude and autocorrelation (whiteness test, CUSUM) as the trigger; and (iii) injecting a no-gradient, linear observer correction to the residual akin to Luenberger gain\u2014none of which are present in prior decoding controls.",
        "Abstract": "We propose Innovation-Observer Decoding (IOD), a lightweight inference-time framework that monitors and repairs incoherence in frozen language models via a control-theoretic observer. We treat the final-layer residual stream as the system state and the emitted token as an input. A small observer f predicts the next residual and logits from current residual and token: (h\u0302_{t+1}, z\u0302_{t+1}) = f(LN(h_t), e(x_t)). The innovation is \u03b5_t = LN(h_{t+1}) \u2212 h\u0302_{t+1} (and optionally logit innovation \u03b6_t = z_{t+1} \u2212 z\u0302_{t+1}). From verified-correct trajectories we estimate innovation covariance \u03a3 and form a whitened score s_t = \u03b5_t^T \u03a3^{-1} \u03b5_t, along with a simple serial-correlation test. Under coherent reasoning, innovations are approximately white and low-norm; before contradictions and hallucinations they spike and become serially dependent. IOD acts only on innovation alarms: (1) innovation-aware lookahead that simulates a depth-1 extension for top-k candidates and selects the token minimizing predicted s_{t+1} under a 95% likelihood floor; and (2) a one-shot observer correction to the residual stream r_t \u2190 r_t + K \u03a3^{-1/2} \u03b5_t (norm-capped), where K is learned offline to reduce next-step innovation without backprop through the base model. No LM weights are updated; the method requires only hidden-state/logit reads and a few extra forward passes on alarm steps. Across math reasoning (GSM8K), NLI (ANLI), and factuality (TruthfulQA), we evaluate innovation as a detector (AUROC, early-warning) and IOD as a controller (verified accuracy, contradiction rate, overhead), comparing to entropy/typical decoding and lookahead-only. Results will test whether an explicit, calibrated innovation observer provides a simple, general, and actionable substrate for online coherence control in LMs.",
        "Experiments": "- Train the observer and calibrate innovation:\n  - Data/models: Llama-2-7B and Mistral-7B. Collect h_t (final-layer residuals) and logits over GSM8K (with a checker), AQuA-RAT, ANLI-R3, StrategyQA, TruthfulQA, plus a synthetic contradiction set.\n  - Observer f: 2-layer MLP (512 hidden) taking [LN(h_t); e(x_t)] and predicting h\u0302_{t+1} (and z\u0302_{t+1}). Train with L2 for residuals and KL for logits on teacher-forced trajectories. Estimate \u03a3 = Cov(\u03b5_t) on verified-correct runs; compute s_t = \u03b5_t^T \u03a3^{-1} \u03b5_t.\n  - Validation: AUROC/PR of max s_t and CV(s) for failure detection (wrong answer by checker, NLI contradiction, untruthful item). Partial correlations controlling for log-prob and length; early-warning lead time (tokens from first s spike to failure). Compare to baselines: entropy, log-prob, ||\u0394h||, FR distance between p_t and p_{t+1}.\n- Innovation-aware decoding (Tier-1):\n  - Trigger: s_t above the 90th percentile of a held-out calibration set or significant lag-1 autocorrelation of \u03b5.\n  - Intervention: Simulate depth-1 lookahead for top-k=5 tokens; compute predicted h\u0302_{t+2}^{(i)} via f and s_{t+1}^{(i)}. Choose token minimizing s_{t+1}^{(i)} subject to \u226595% of max log-prob. Evaluate verified accuracy (GSM8K exact match; AQuA, ANLI, StrategyQA accuracy; TruthfulQA truthful score), internal contradiction rate (NLI-based), and latency overhead.\n- Observer correction (Tier-2):\n  - If alarms persist for two steps, apply a norm-capped correction r_t \u2190 r_t + K \u03a3^{-1/2} \u03b5_t before producing next logits (no gradients). Learn K offline via ridge regression to minimize E[||\u03b5_{t+1}||^2] on training splits. Ablate gain magnitude and norm caps; compare against logit-space temperature smoothing and against Jacobian-based steering (heavier baseline) under equal compute.\n- Ablations and variants:\n  - Residual-only vs residual+logit innovation; different \u03a3 estimators (diagonal vs full). Replace MLP observer with a linear autoregression. Remove Tier-2 to test lookahead-only gains. Vary k and likelihood floors.\n- Transfer and robustness:\n  - Train f, \u03a3, K on Llama-2-7B; test zero-shot on Mistral-7B (optional LN rescaling). Report AUROC shift and downstream gains; light percentile recalibration if needed.\n  - Adversarial prompts with subtle contradictions; benign enumerations/lists to assess false alarms. Measure trigger rate and specificity.\n- Efficiency:\n  - Profile ms/token overhead, alarm frequency, and total slowdown; target <1.6\u00d7 on alarm steps and <5% overall.",
        "Risk Factors and Limitations": "- Proxy fidelity: Innovation spikes may partly reflect difficulty/novelty rather than incoherence; control via partial correlations, difficulty-stratified analyses, and qualitative audits.\n- Calibration/transfer: \u03a3, thresholds, and gain K may be model-specific; cross-model transfer can require light recalibration or feature whitening.\n- Overhead: Lookahead adds k extra forwards on alarm steps; keep k small and alarms sparse. Observer correction is cheap but must be norm-capped.\n- Access constraints: Requires hidden-state/logit reads; closed APIs may be incompatible.\n- Over-conservatism: Minimizing innovation may bias toward safe tokens; enforce likelihood floors and allow rare high-innovation steps.\n- Scope: The method targets local coherence; very long-horizon consistency may still need planning or external verifiers."
    }
]